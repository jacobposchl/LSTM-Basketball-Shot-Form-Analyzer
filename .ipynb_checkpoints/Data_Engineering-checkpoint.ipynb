{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b908bd01-01ce-47a7-b08a-a5ba9196101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset: Datasets\\long_vid_entire_dataset.csv\n",
      "All required columns are present.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Paths for datasets\n",
    "datasets_folder = 'Datasets'\n",
    "combined_dataset_filename = 'long_vid_entire_dataset.csv'\n",
    "combined_dataset_path = os.path.join(datasets_folder, combined_dataset_filename)\n",
    "\n",
    "# Step 1: Load the combined dataset\n",
    "try:\n",
    "    combined_df = pd.read_csv(combined_dataset_path)\n",
    "    print(f\"Successfully loaded dataset: {combined_dataset_path}\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Combined dataset not found at: {combined_dataset_path}\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The file is empty.\")\n",
    "    exit()\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: The file could not be parsed.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Verify required columns exist\n",
    "required_columns = ['shot_id', 'is_shot', 'frame']\n",
    "missing_columns = [col for col in required_columns if col not in combined_df.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns in the dataset: {missing_columns}\")\n",
    "else:\n",
    "    print(\"All required columns are present.\")\n",
    "\n",
    "df = combined_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1068fab9-77df-4346-a596-e44f48be279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the ball_postiion column into two -> ball_pos_x and ball_pos_y\n"
     ]
    }
   ],
   "source": [
    "### Split ball_position column\n",
    "\n",
    "# Ensure 'sports_ball_positions' column exists\n",
    "if 'sports_ball_positions' in df.columns:\n",
    "    # Split the 'sports_ball_positions' column into two new columns\n",
    "    df[['ball_pos_x', 'ball_pos_y']] = df['sports_ball_positions'].str.split(',', expand=True)\n",
    "\n",
    "    # Convert the new columns to float type for numerical operations\n",
    "    df['ball_pos_x'] = pd.to_numeric(df['ball_pos_x'], errors='coerce')\n",
    "    df['ball_pos_y'] = pd.to_numeric(df['ball_pos_y'], errors='coerce')\n",
    "\n",
    "    # Drop the original 'sports_ball_positions' column\n",
    "    df.drop(columns=['sports_ball_positions'], inplace=True)\n",
    "\n",
    "\n",
    "print(\"Split the ball_postiion column into two -> ball_pos_x and ball_pos_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b07be49-a533-471a-b286-20db458a4e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "### INTERPOLATE VALUES\n",
    "\n",
    "def optimized_interpolate_zeros(data, columns):\n",
    "    \n",
    "    df_interpolated = data.copy()\n",
    "    df_interpolated[columns] = df_interpolated[columns].replace(0, np.nan)\n",
    "    df_interpolated[columns] = df_interpolated[columns].interpolate(method='linear', limit_direction='both')\n",
    "    return df_interpolated\n",
    "\n",
    "# Apply the function to all columns in the DataFrame except specified columns\n",
    "columns_to_exclude = ['frame', 'video', 'is_shot', 'shot_id', 'shot_invalid', 'make']\n",
    "columns_to_interpolate = [col for col in df.columns if col not in columns_to_exclude]\n",
    "\n",
    "# Check if columns_to_interpolate is not empty\n",
    "if not columns_to_interpolate:\n",
    "    print(\"No columns available for interpolation after excluding specified columns.\")\n",
    "    df = df.copy()\n",
    "else:\n",
    "    df = optimized_interpolate_zeros(df, columns_to_interpolate)\n",
    "    print(\"Interpolation completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14470841-834a-48e3-9943-0156606f089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110, 18, 40, 47, 45, 26, 40, 34, 32, 36, 34, 33, 35, 85, 39, 15, 35, 52, 37, 39, 41, 35, 38, 37, 41, 37]\n",
      "\n",
      "Sample Subdataset Information of First 30 Motions:\n"
     ]
    }
   ],
   "source": [
    "### Compute Start and End frames of shot motions\n",
    "\n",
    "\n",
    "#Step 3: Identify all (start_frame, end_frame) pairs and compute shot_id_length for each\n",
    "sequence_info = []  # List to store tuples of (shot_id, start_frame, end_frame, shot_id_length)\n",
    "\n",
    "unique_shot_ids = df['shot_id'].unique()\n",
    "\n",
    "for shot_id in unique_shot_ids:\n",
    "    # Extract rows for the current shot_id\n",
    "    shot_df = df[df['shot_id'] == shot_id].copy().reset_index(drop=True)\n",
    "\n",
    "    # Identify the start and end indices of each 'is_shot' == True sequence\n",
    "    shot_df['is_shot_shift'] = shot_df['is_shot'].shift(1, fill_value=0)\n",
    "    shot_df['is_shot_next'] = shot_df['is_shot'].shift(-1, fill_value=0)\n",
    "\n",
    "    # A sequence starts where 'is_shot' changes from 0 to 1\n",
    "    start_indices = shot_df[(shot_df['is_shot'] == 1) & (shot_df['is_shot_shift'] == 0)].index.tolist()\n",
    "    # A sequence ends where 'is_shot' changes from 1 to 0\n",
    "    end_indices = shot_df[(shot_df['is_shot'] == 1) & (shot_df['is_shot_next'] == 0)].index.tolist()\n",
    "\n",
    "    # Ensure that every start has a corresponding end\n",
    "    if len(start_indices) != len(end_indices):\n",
    "        # Handle cases where a sequence starts but doesn't end\n",
    "        if len(start_indices) > len(end_indices):\n",
    "            end_indices.append(len(shot_df) - 1)\n",
    "\n",
    "    # Iterate through each sequence\n",
    "    for start_idx, end_idx in zip(start_indices, end_indices):\n",
    "        # Get the actual frame numbers\n",
    "        start_frame = shot_df.loc[start_idx, 'frame']\n",
    "        end_frame = shot_df.loc[end_idx, 'frame']\n",
    "\n",
    "        # Calculate shot_id_length\n",
    "        shot_id_length = end_frame - start_frame\n",
    "\n",
    "        # Append the information\n",
    "        sequence_info.append((shot_id, start_frame, end_frame, shot_id_length))\n",
    "\n",
    "# Step 4: Determine max_length as the maximum shot_id_length across all sequences\n",
    "shot_id_lengths = [info[3] for info in sequence_info]\n",
    "print(shot_id_lengths)\n",
    "\n",
    "# Compute Q1 (25th percentile) and Q3 (75th percentile)\n",
    "Q1 = np.percentile(shot_id_lengths, 25)\n",
    "Q3 = np.percentile(shot_id_lengths, 75)\n",
    "\n",
    "# Calculate IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Determine bounds for non-outlier lengths\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter shot_id_lengths to exclude outliers\n",
    "filtered_lengths = [length for length in shot_id_lengths if lower_bound <= length <= upper_bound]\n",
    "\n",
    "# Calculate max_length as the maximum of the filtered lengths\n",
    "max_length = max(filtered_lengths) if filtered_lengths else 0\n",
    "# Step 5: Calculate the length of each subdataset based on the criteria\n",
    "# Create a DataFrame to store the results\n",
    "subdataset_info = []  # List to store tuples of (shot_id, adjusted_start_frame, end_frame, subdataset_length)\n",
    "\n",
    "for (shot_id, start_frame, end_frame, shot_id_length) in sequence_info:\n",
    "    # Calculate the adjusted start frame\n",
    "    adjusted_start_frame = end_frame - max_length\n",
    "    if adjusted_start_frame < 0:\n",
    "        adjusted_start_frame = 0  # Adjust based on your frame numbering (0 or 1)\n",
    "\n",
    "    # Calculate the length of the subdataset\n",
    "    subdataset_length = end_frame - adjusted_start_frame + 1  # +1 if inclusive\n",
    "\n",
    "    # Append the information\n",
    "    subdataset_info.append((shot_id, adjusted_start_frame, end_frame, subdataset_length))\n",
    "\n",
    "\n",
    "# Convert the list to a DataFrame for better visualization and analysis\n",
    "subdataset_df = pd.DataFrame(subdataset_info, columns=['shot_id', 'adjusted_start_frame', 'end_frame', 'frames'])\n",
    "\n",
    "\n",
    "# Optional: Display first few rows of the subdataset_info\n",
    "print(\"\\nSample Subdataset Information of First 30 Motions:\")\n",
    "#print(subdataset_df.head(30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3ed6cef-4ef1-43a0-9b96-f37dedc8d463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Datasets of Each Shot Motion and Dropped Un-needed Columns\n"
     ]
    }
   ],
   "source": [
    "### Create subdatasets\n",
    "\n",
    "subdatasets = []\n",
    "for row in subdataset_df.itertuples(index=False):\n",
    "    start_frame = row.adjusted_start_frame\n",
    "    end_frame = row.end_frame\n",
    "    shot_id = row.shot_id\n",
    "\n",
    "    sub_df = df[\n",
    "        (df['frame'] >= start_frame) &  # Corrected variable name\n",
    "        (df['frame'] <= end_frame)\n",
    "    ].copy()\n",
    "\n",
    "    # Append the subset to subdatasets list\n",
    "    subdatasets.append(sub_df)\n",
    "\n",
    "for df in subdatasets:\n",
    "    #Drop Un-needed columns\n",
    "    df.drop(columns=['video', 'is_shot', 'shot_invalid'], inplace=True)\n",
    "\n",
    "\n",
    "print(\"Created Datasets of Each Shot Motion and Dropped Un-needed Columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83445823-3c35-479b-9e66-af887787d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "made = []\n",
    "missed = []\n",
    "\n",
    "for df in standardized_dfs:\n",
    "    if (df[\"make\"] == True).any():  # If at least one row is True\n",
    "        # Make all values in the \"make\" column set to True\n",
    "        df[\"make\"] = 1\n",
    "        \n",
    "        # Find the maximum value in the \"shot_id\" column\n",
    "        max_shot_id = df[\"shot_id\"].max()\n",
    "\n",
    "        # Set all values in the \"shot_id\" column to the maximum value\n",
    "        df[\"shot_id\"] = max_shot_id\n",
    "\n",
    "        # Append the modified DataFrame to the 'made' list\n",
    "        made.append(df)\n",
    "        \n",
    "    elif (df[\"make\"] == False).any():  # If at least one row is False\n",
    "        # Make all values in the \"make\" column set to False\n",
    "        df[\"make\"] = 0\n",
    "\n",
    "        # Find the maximum value in the \"shot_id\" column\n",
    "        max_shot_id = df[\"shot_id\"].max()\n",
    "\n",
    "        # Set all values in the \"shot_id\" column to the maximum value\n",
    "        df[\"shot_id\"] = max_shot_id\n",
    "\n",
    "        # Append the modified DataFrame to the 'missed' list\n",
    "        missed.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f63467-cf58-4956-80ad-dcde5b71f864",
   "metadata": {},
   "source": [
    "Now, there are two lists:\n",
    "\n",
    "Made list: All shot motions with Good form.\n",
    "\n",
    "Missed list: All shot motions with Bad form.\n",
    "\n",
    "Goal: Combine these two lists into a single big dataframe: \"data_df\"\n",
    "\n",
    "First, lets visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf9f09d8-22c2-441f-9b52-710bb33d2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plots = False\n",
    "\n",
    "if show_plots:\n",
    "    # -----------------------------------------------\n",
    "    # Step 1: Prepare Labels and Colors\n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Generate labels for each DataFrame in 'made' and 'missed'\n",
    "    made_labels = [f'Made {i+1}' for i in range(len(made))]\n",
    "    missed_labels = [f'Missed {i+1}' for i in range(len(missed))]\n",
    "    \n",
    "    # Assign colors: green for 'made' and red for 'missed'\n",
    "    made_colors = ['green'] * len(made)\n",
    "    missed_colors = ['red'] * len(missed)\n",
    "    \n",
    "    # Combine the lists for easier processing\n",
    "    all_dfs = made + missed\n",
    "    all_labels = made_labels + missed_labels\n",
    "    all_colors = made_colors + missed_colors\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    # Step 2: Plotting All DataFrames\n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Check if there are any DataFrames to plot\n",
    "    if not all_dfs:\n",
    "        raise ValueError(\"Both 'made' and 'missed' lists are empty. Please check your data processing steps.\")\n",
    "    \n",
    "    # Select only numeric columns from the first DataFrame (assuming all have the same columns)\n",
    "    numeric_columns = all_dfs[0].select_dtypes(include=['number']).columns\n",
    "    \n",
    "    # Loop through each numeric feature and plot for all datasets\n",
    "    for feature in numeric_columns:\n",
    "        plt.figure(figsize=(24, 12))\n",
    "        \n",
    "        for df, label, color in zip(all_dfs, all_labels, all_colors):\n",
    "            x = df.index\n",
    "            y = df[feature]\n",
    "            plt.plot(\n",
    "                x, y,\n",
    "                marker='o',\n",
    "                linestyle='--',\n",
    "                color=color,\n",
    "                label=label\n",
    "            )\n",
    "        \n",
    "        # Set the title, labels, and grid\n",
    "        plt.title(f'{feature} Over Index', fontsize=16)\n",
    "        plt.xlabel('Frame', fontsize=14)\n",
    "        plt.ylabel(feature, fontsize=14)\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        \n",
    "        # Handle the legend to avoid duplicate labels\n",
    "        handles, labels_ = plt.gca().get_legend_handles_labels()\n",
    "        unique_labels = {}\n",
    "        for handle, label in zip(handles, labels_):\n",
    "            if label not in unique_labels:\n",
    "                unique_labels[label] = handle\n",
    "        plt.legend(unique_labels.values(), unique_labels.keys(), title='Datasets', fontsize=12, title_fontsize=12, loc='best')\n",
    "        \n",
    "        # Optimize layout\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa78108c-fd98-49cd-95a4-e8443d049ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both lists into a single list\n",
    "combined_list = made + missed\n",
    "\n",
    "data_df = pd.concat(combined_list, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf04739e-22e0-4036-b504-16ce46e24e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(18)\n",
    "\n",
    "# Step 0: Sort data_df by 'shot_id' and 'frame' to ensure frame order within each shot_id\n",
    "data_df = data_df.sort_values(by=['shot_id', 'frame']).reset_index(drop=True)\n",
    "\n",
    "# Step 1: Identify Unique shot_ids\n",
    "unique_shot_ids = data_df['shot_id'].unique()\n",
    "\n",
    "# Step 2: Shuffle the list of unique shot_ids\n",
    "shuffled_shot_ids = np.random.permutation(unique_shot_ids)\n",
    "\n",
    "# Step 3: Create a list of DataFrames ordered by shuffled shot_ids\n",
    "shuffled_dfs = [data_df[data_df['shot_id'] == shot_id] for shot_id in shuffled_shot_ids]\n",
    "\n",
    "# Step 4: Concatenate the shuffled DataFrames into one big DataFrame\n",
    "shuffled_data_df = pd.concat(shuffled_dfs, ignore_index=True)\n",
    "\n",
    "# Replace the original data_df with shuffled_data_df\n",
    "data_df = shuffled_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6c5bb-7d18-4b80-8ca2-a268b90878e6",
   "metadata": {},
   "source": [
    "Now, we have a big shuffled dataset, each shot motion is grouped together and in a random order based on the shot id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ccbea94f-6b77-43cc-a159-9f1d1f75593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique shot_ids in shuffled order: [23  1 10 13  7  2  4 21 12  8 16  5 14 26 22 17 24 25  9  3 19 15 18  6\n",
      " 20 11]\n",
      "\n",
      "Total shot_ids: 26\n",
      "Training shot_ids: 18\n",
      "Validation shot_ids: 3\n",
      "Testing shot_ids: 5\n",
      "\n",
      "Assigned shot_ids:\n",
      "Training shot_ids: [23  1 10 13  7  2  4 21 12  8 16  5 14 26 22 17 24 25]\n",
      "Validation shot_ids: [ 9  3 19]\n",
      "Testing shot_ids: [15 18  6 20 11]\n"
     ]
    }
   ],
   "source": [
    "# Extract unique shot_ids in the order they appear in data_df\n",
    "unique_shot_ids = data_df['shot_id'].unique()\n",
    "print(\"Unique shot_ids in shuffled order:\", unique_shot_ids)\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.7\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Total number of unique shot_ids\n",
    "total_shots = len(unique_shot_ids)\n",
    "\n",
    "# Calculate the number of shot_ids for each split\n",
    "train_end = int(train_ratio * total_shots)\n",
    "validation_end = train_end + int(validation_ratio * total_shots)\n",
    "\n",
    "print(f\"\\nTotal shot_ids: {total_shots}\")\n",
    "print(f\"Training shot_ids: {train_end}\")\n",
    "print(f\"Validation shot_ids: {validation_end - train_end}\")\n",
    "print(f\"Testing shot_ids: {total_shots - validation_end}\")\n",
    "\n",
    "\n",
    "# Assign shot_ids to each split\n",
    "train_shot_ids = unique_shot_ids[:train_end]\n",
    "validation_shot_ids = unique_shot_ids[train_end:validation_end]\n",
    "test_shot_ids = unique_shot_ids[validation_end:]\n",
    "\n",
    "print(\"\\nAssigned shot_ids:\")\n",
    "print(f\"Training shot_ids: {train_shot_ids}\")\n",
    "print(f\"Validation shot_ids: {validation_shot_ids}\")\n",
    "print(f\"Testing shot_ids: {test_shot_ids}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b8950aa-76fa-4264-b8dd-2b9771db2f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split Sizes:\n",
      "Training set: 18 shot_ids, 864 rows\n",
      "Validation set: 3 shot_ids, 144 rows\n",
      "Testing set: 5 shot_ids, 240 rows\n"
     ]
    }
   ],
   "source": [
    "# Create boolean masks for each split\n",
    "train_mask = data_df['shot_id'].isin(train_shot_ids)\n",
    "validation_mask = data_df['shot_id'].isin(validation_shot_ids)\n",
    "test_mask = data_df['shot_id'].isin(test_shot_ids)\n",
    "\n",
    "# Create the splits\n",
    "train_df = data_df[train_mask].reset_index(drop=True)\n",
    "validation_df = data_df[validation_mask].reset_index(drop=True)\n",
    "test_df = data_df[test_mask].reset_index(drop=True)\n",
    "\n",
    "# Display the number of shot_ids and rows in each split\n",
    "print(\"\\nSplit Sizes:\")\n",
    "print(f\"Training set: {train_df['shot_id'].nunique()} shot_ids, {len(train_df)} rows\")\n",
    "print(f\"Validation set: {validation_df['shot_id'].nunique()} shot_ids, {len(validation_df)} rows\")\n",
    "print(f\"Testing set: {test_df['shot_id'].nunique()} shot_ids, {len(test_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca3ba2-6f8a-46c1-a5e6-4e13ff5b513b",
   "metadata": {},
   "source": [
    "Now that I have Train, Test, and Valid Sets, I need to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "384c315d-a050-4b22-a901-72536cebc11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization completed after splitting the data.\n"
     ]
    }
   ],
   "source": [
    "# After splitting the data\n",
    "# Assuming train_df, validation_df, and test_df have been created\n",
    "\n",
    "columns_to_exclude = ['frame', 'make', 'shot_id']\n",
    "numeric_columns = train_df.select_dtypes(include = [\"float64\", \"int\"]).columns.tolist()\n",
    "columns_to_scale = [col for col in numeric_columns if col not in columns_to_exclude]\n",
    "\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler.fit(train_df[columns_to_scale])\n",
    "\n",
    "# Transform the training, validation, and test sets\n",
    "train_df[columns_to_scale] = scaler.transform(train_df[columns_to_scale])\n",
    "validation_df[columns_to_scale] = scaler.transform(validation_df[columns_to_scale])\n",
    "test_df[columns_to_scale] = scaler.transform(test_df[columns_to_scale])\n",
    "\n",
    "print(\"Normalization completed after splitting the data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35cc24f-10e1-49f8-982c-765af68d5d12",
   "metadata": {},
   "source": [
    "Now, I have three dataframes: train, test, and valid. I would like to convert these into three separate .npy files. Each with the shape of (Shot Motion, Frame, Features).\n",
    "\n",
    "This allow me to input these files into LSTM_Model.py and start training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f59e9c4-e333-40e2-ab73-adc60d2152fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_tensor(df, feature_columns):\n",
    "    grouped = df.groupby('shot_id')\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for shot_id, group in grouped:\n",
    "        # Sort the group by 'frame' to maintain sequential order\n",
    "        sorted_group = group.sort_values('frame')\n",
    "        \n",
    "        # Extract feature values\n",
    "        features = sorted_group[feature_columns].values  # Shape: (num_frames, num_features)\n",
    "        \n",
    "        # Extract label (assumes 'make' is consistent within a shot)\n",
    "        label = sorted_group['make'].iloc[0]\n",
    "        \n",
    "        data_list.append(features)\n",
    "        labels_list.append(label)\n",
    "    \n",
    "    # Convert lists to NumPy arrays\n",
    "    data_tensor = np.array(data_list)      # Shape: (num_shots, num_frames, num_features)\n",
    "    labels = np.array(labels_list)        # Shape: (num_shots,)\n",
    "    \n",
    "    return data_tensor, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ec5f18e-fa93-4eff-8201-742379d58e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data tensor shape: (18, 48, 40)\n",
      "Training labels shape: (18,)\n",
      "Validation data tensor shape: (3, 48, 40)\n",
      "Validation labels shape: (3,)\n",
      "Testing data tensor shape: (5, 48, 40)\n",
      "Testing labels shape: (5,)\n"
     ]
    }
   ],
   "source": [
    "# Define columns to exclude\n",
    "columns_to_exclude = ['frame', 'make', 'shot_id']\n",
    "\n",
    "# Assuming all DataFrames have the same columns, use one to determine feature columns\n",
    "feature_columns = [col for col in train_df.columns if col not in columns_to_exclude]\n",
    "\n",
    "output_folder = 'Datasets'  # Ensure this folder exists\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Convert training data\n",
    "train_data_tensor, train_labels = dataframe_to_tensor(train_df, feature_columns)\n",
    "print(f\"Training data tensor shape: {train_data_tensor.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "\n",
    "# Convert validation data\n",
    "validation_data_tensor, validation_labels = dataframe_to_tensor(validation_df, feature_columns)\n",
    "print(f\"Validation data tensor shape: {validation_data_tensor.shape}\")\n",
    "print(f\"Validation labels shape: {validation_labels.shape}\")\n",
    "\n",
    "# Convert testing data\n",
    "test_data_tensor, test_labels = dataframe_to_tensor(test_df, feature_columns)\n",
    "print(f\"Testing data tensor shape: {test_data_tensor.shape}\")\n",
    "print(f\"Testing labels shape: {test_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d5fb9f7-721a-41d2-8062-a622aece51a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.87616941e-01 -1.14171804e+00 -1.63398057e-01 ... -8.85143742e-01\n",
      "  -8.16187438e-01  1.56746645e+00]\n",
      " [-9.71618105e-01 -1.04110756e+00  3.95185975e-04 ... -8.93214041e-01\n",
      "  -7.85031073e-01  1.55656012e+00]\n",
      " [-9.05337215e-01 -9.29730653e-01  8.46577581e-02 ... -9.10285827e-01\n",
      "  -7.53874708e-01  1.54565379e+00]\n",
      " ...\n",
      " [-5.31078741e-01 -4.14426811e-01 -1.70468415e-01 ...  1.15865948e+00\n",
      "  -7.07295013e-01 -1.23358511e+00]\n",
      " [-6.95885198e-02  3.07790581e-01  7.46475650e-01 ...  1.15089958e+00\n",
      "  -3.93625365e-01 -1.45194634e+00]\n",
      " [ 8.54478152e-02  5.80587957e-02  2.33193152e-01 ...  1.14981319e+00\n",
      "   6.72162985e-02 -1.70179522e+00]]\n",
      "[[-0.82077194 -0.89656508  0.06174494 ... -0.90656107 -0.41790627\n",
      "   1.35589481]\n",
      " [-0.6837343  -0.70425428  0.20310289 ... -0.9124586  -0.40427923\n",
      "   1.35067863]\n",
      " [-0.57840863 -0.55587549  0.14999273 ... -0.92471925 -0.39065219\n",
      "   1.34546245]\n",
      " ...\n",
      " [-0.60145457 -0.00567355 -0.20752195 ...  1.1571075  -0.54575267\n",
      "  -1.22215105]\n",
      " [-0.16472445  0.80774916  0.70495358 ...  1.15058918 -0.15701042\n",
      "  -1.48402035]\n",
      " [-0.02102063  0.29875668  0.21432054 ...  1.15152037  0.37419631\n",
      "  -1.75028736]]\n",
      "[1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(test_data_tensor[0])\n",
    "print(test_data_tensor[1])\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "421b52d2-bc62-4c04-8770-1df859cfae5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train data to Datasets\\train_data.npy\n",
      "Saved train labels to Datasets\\train_labels.npy\n",
      "Saved validation data to Datasets\\validation_data.npy\n",
      "Saved validation labels to Datasets\\validation_labels.npy\n",
      "Saved test data to Datasets\\test_data.npy\n",
      "Saved test labels to Datasets\\test_labels.npy\n"
     ]
    }
   ],
   "source": [
    "def save_tensor_and_labels(data_tensor, labels, dataset_type, output_dir='Datasets'):\n",
    "    \"\"\"\n",
    "    Saves the data tensor and labels as .npy files.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_tensor (np.ndarray): 3D data tensor.\n",
    "    - labels (np.ndarray): 1D labels array.\n",
    "    - dataset_type (str): Type of dataset ('train', 'validation', 'test').\n",
    "    - output_dir (str): Directory to save the .npy files.\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(output_dir, f\"{dataset_type}_data.npy\")\n",
    "    labels_path = os.path.join(output_dir, f\"{dataset_type}_labels.npy\")\n",
    "    \n",
    "    np.save(data_path, data_tensor)\n",
    "    np.save(labels_path, labels)\n",
    "    \n",
    "    print(f\"Saved {dataset_type} data to {data_path}\")\n",
    "    print(f\"Saved {dataset_type} labels to {labels_path}\")\n",
    "\n",
    "# Save all datasets\n",
    "save_tensor_and_labels(train_data_tensor, train_labels, 'train', output_dir=output_folder)\n",
    "save_tensor_and_labels(validation_data_tensor, validation_labels, 'validation', output_dir=output_folder)\n",
    "save_tensor_and_labels(test_data_tensor, test_labels, 'test', output_dir=output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a3809-46b4-4648-b124-07c1982ff594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
